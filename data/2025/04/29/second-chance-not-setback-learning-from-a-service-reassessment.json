{
    "url": "https://digitaltrade.blog.gov.uk/2025/04/29/second-chance-not-setback-learning-from-a-service-reassessment/",
    "title": "Second chance, not setback: learning from a service reassessment",
    "authors": [
        "Clare Hill, Senior Content Designer"
    ],
    "categories": [
        "Products"
    ],
    "pub_date": "2025-04-29T10:27:30+01:00",
    "content": [
        {
            "heading": 3,
            "text": "Clare Hill"
        },
        {
            "text": "I\u2019m part of the team working on Expand your business in the UK, which recently met the standard for its beta service assessment. This Department for Business and Trade (DBT) service provides foreign direct investors with guidance and data on the United Kingdom business environment and what\u2019s involved in setting up shop here with a business of their type and sector.\u00a0"
        },
        {
            "text": "A service assessment is a peer review process that checks whether a service meets the government Service Standard, so that it is user-centred and fit for purpose. Services usually cannot progress to the next stage of development, for example from private to public beta, unless they are assessed."
        },
        {
            "text": "We didn\u2019t meet the standard the first time around. It was hard as a team not to feel disappointed \u2013 as if it were an exam and that we\u2019d failed (it wasn\u2019t, and we didn\u2019t). However, only after going over the resulting \u2018report card\u2019 from the first assessment did it become clear how useful and constructive the feedback from the process could be.\u00a0\u00a0"
        },
        {
            "heading": 3,
            "text": "Clarity begins at home"
        },
        {
            "text": "For services likely to handle over 100,000 annual transactions or cut across departments, assessments are run by the Central Digital and Data Office. Otherwise, they\u2019re managed by the department that owns the service.DBT implemented its own service assessment model 3 years ago, and on the one hand the process is relatively established. On the other hand, it means many teams and team members are experiencing the process for the first time. This was the case for me, and it wasn\u2019t necessarily clear what it all meant at first.The format the first time around was quite free form. We had broad themes or talking points to pull together evidence on and present. For example, User Experience improvements based on our user research findings and analytics, and identification of service design challenges and describing our solutions.\u00a0"
        },
        {
            "text": "We\u2019re a big team and we ended up with a similarly big slide deck to present to the half-dozen or so assessors. They were drawn from across Digital, Data and Technology (DDaT) in DBT, including analysts from the Portfolio Management Office, and user-centred design professionals. It was a mix of familiar and unfamiliar faces.\u00a0"
        },
        {
            "heading": 3,
            "text": "The report card"
        },
        {
            "text": "The assessment panel did a brilliant and patient job of finding and highlighting the areas we needed to improve on most, from the large amount of information we presented. Those areas where the service didn\u2019t meet the standard were mainly concerned with knowing our users better and ensuring it was properly integrated with other services.\u00a0 We needed to build a consistent experience for users entering from different journey starting points. The assessment report format takes each of the 14 points in the Service Standard in turn, listing what the team did well and what they need to explore."
        },
        {
            "text": "The assessment panel highlighted areas that we needed to explore further. Prior to the assessment it was a case of \u2018not knowing what we didn\u2019t know\u2019. Now that we did, we had a detailed blueprint for improvement."
        },
        {
            "heading": 3,
            "text": "Acting on the recommendations"
        },
        {
            "text": "These lists of practical recommendations to address areas where the service could be better were quite galvanising for the team. Personally, I felt a real contrast between putting together the case for the initial assessment and the work needed for what would become the reassessment. The former felt abstract while the latter seemed concrete and much more actionable.\u00a0"
        },
        {
            "text": "There were several months between the original service assessment and the reassessment, during which time we metaphorically picked ourselves up as a team. Many of us started to see the report recommendations as an opportunity to overhaul ways of working, reframe user needs and come up with new ways to reach out to returning and past service users.We welcomed some new team members in the meantime, which naturally helped spark some fresh approaches and solutions. Particular credit must go to Sam Harbison, a senior user researcher who joined a couple of months after the service assessment. She was instrumental in facilitating and pulling together our successful reassessment pitch, as well as reimagining user research practice in our team.There were some other silver linings too: the recommendations gave us extra backing and confidence to develop and deploy some changes to the service that had already been in the works, such as a completely redesigned start page.\u00a0"
        },
        {
            "heading": 3,
            "text": "Time to reconsider"
        },
        {
            "text": "The reassessment meeting was much more relaxed and manageable than the original assessment. It was a same-sized team, but with a third of the slides, and a third of the number of assessors. I felt we were much better prepared this time around, we\u2019d worked hard to weave clear storytelling into the specific points that we were addressing. It was more of a discussion with critical friends than an elaborate presentation to a formidable panel.\u00a0"
        },
        {
            "text": "Not only was it more relaxed, but we also weren\u2019t waiting nervously for days for a massive report to be written up. We received the good news just hours later \u2013 that we\u2019d met the standard!"
        },
        {
            "heading": 3,
            "text": "Lessons in disguise"
        },
        {
            "text": "My biggest takeaway from the whole process is that service assessments really are a positive opportunity to reset and refocus a team and its mission or project. Another learning is that the more structured and organised approach the team took with reassessment can be applied to future service assessments. This may well help with achieving a green or amber assessment rating in one go.\u00a0"
        },
        {
            "text": "Service assessments have a tendency to be likened to tests or job interviews. But it\u2019s much more helpful to see them as a chance to showcase a team\u2019s hard work and receive\u00a0constructive recommendations that lead to meaningful improvements to the service."
        },
        {
            "text": "Failure never comes into it."
        },
        {
            "heading": 3,
            "text": "Ready to join us?"
        },
        {
            "text": "Digital, Data, and Technology are always looking for a wide range of skills. Explore\u00a0current opportunities\u00a0and discover how you can make a difference."
        }
    ]
}